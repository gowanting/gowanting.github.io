<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="description" content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos."> <meta name="keywords" content="Nerfies, D-NeRF, NeRF"> <meta name="viewport" content="width=device-width, initial-scale=1"> <title>SpatialGrounding</title> <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-PYVRSFMDRL");</script> <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet"> <link rel="stylesheet" href="../SpatialGrounding/css/bulma.min.css"> <link rel="stylesheet" href="../SpatialGrounding/css/bulma-carousel.min.css"> <link rel="stylesheet" href="../SpatialGrounding/css/bulma-slider.min.css"> <link rel="stylesheet" href="../SpatialGrounding/css/fontawesome.all.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"> <link rel="stylesheet" href="../SpatialGrounding/css/index.css"> <link rel="icon" href="../SpatialGrounding/images/logo.png"> <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script> <script defer src="../SpatialGrounding/js/fontawesome.all.min.js"></script> <script src="../SpatialGrounding/js/bulma-carousel.min.js"></script> <script src="../SpatialGrounding/js/bulma-slider.min.js"></script> <script src="../SpatialGrounding/js/index.js"></script> </head> <body> <nav class="navbar" role="navigation" aria-label="main navigation"> <div class="navbar-brand"> <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false"> <span aria-hidden="true"></span> <span aria-hidden="true"></span> <span aria-hidden="true"></span> </a> </div> <div class="navbar-menu"> <div class="navbar-start" style="flex-grow: 1; justify-content: center;"> <a class="navbar-item" href="https://gowanting.github.io/"> <span class="icon"> <i class="fas fa-home"></i> </span> </a> </div> </div> </nav> <section class="hero"> <div class="hero-body"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column has-text-centered"> <h1 class="title is-1 publication-title" style="display: flex; align-items: center; justify-content: center; gap: 15px;"> <img src="../SpatialGrounding/images/logo.png" width="60" alt="Logo"> <span class="dnerf">SpatialGrounding</span> </h1> <div class="is-size-5 publication-authors"> <span class="author-block"> <a href="https://gowanting.github.io/">Wanting Xu</a><sup>1</sup>,</span> <span class="author-block"> Lingbing Zeng<sup>1</sup>,</span> <span class="author-block"> <a href="https://scholar.google.com/citations?user=luo2KJ8AAAAJ&amp;hl=zh-CN&amp;oi=ao" rel="external nofollow noopener" target="_blank">Ran Cheng</a><sup>1</sup>, </span> <span class="author-block"> Lige Liu<sup>1</sup>,</span> <span class="author-block"> <a href="https://scholar.google.com/citations?user=cO9hYf4AAAAJ&amp;hl=zh-CN&amp;oi=sra" rel="external nofollow noopener" target="_blank">Tao Sun</a><sup>1</sup> </span> </div> <div class="is-size-5 publication-authors"> <span class="author-block"><sup>1</sup>Midea Group</span> </div> <div class="column has-text-centered"> <div class="publication-links"> <span class="link-block"> <a href="https://arxiv.org/abs/" class="button is-normal is-rounded is-light is-static" rel="external nofollow noopener" target="_blank"> <span class="icon"> <i class="ai ai-arxiv"></i> </span> <span>TechReport (Coming Soon)</span> </a></span> <span class="link-block"> <a href="https://github.com/gowanting/SpatialGrounding" class="external-link button is-normal is-rounded is-dark" rel="external nofollow noopener" target="_blank"> <span class="icon"> <i class="fab fa-github"></i> </span> <span>Code</span> </a> </span> </div> </div> </div> </div> </div> <section class="hero teaser"> <div class="container is-max-desktop"> <div class="hero-body" style="padding-top: 1rem; padding-bottom: 1rem;"> <img id="teaser" src="../SpatialGrounding/images/2.png" alt="Teaser" style="width: 100%; height: auto;"> <h2 class="subtitle has-text-centered"> </h2> </div> </div> </section> <section class="section"> <div class="container is-max-desktop"> <div class="columns is-centered has-text-centered"> <div class="column is-four-fifths"> <h2 class="title is-3">Abstract</h2> <div class="content has-text-justified"> <p>We present <span class="dnerf">SpatialGrounding</span>, a novel multi-modal framework designed to <strong>ground high-level semantic concepts within geometrically precise 3D reconstructions</strong> of indoor environments, specifically for mobile robot spatial understanding. Our approach addresses critical challenges by fusing heterogeneous sensor data, integrating LDS SLAM 2D pose priors and 2D occupancy maps with deep learning-based visual matching from monocular image sequences.</p> <p>The system's core establishes a high-fidelity geometric foundation by leveraging pose priors from SLAM to solve scale ambiguity and enforce global consistency. An intelligent image pair selection strategy, guided by the robot's 2D occupancy map, significantly reduces computational overhead. This spatially-constrained data is processed using <strong>MASt3R</strong> for robust feature matching and <strong>GLOMAP</strong> for global optimization. Subsequently, the framework achieves <strong>semantic grounding</strong> by employing <strong>FC-CLIP</strong> to lift 2D visual cues into dense 3D semantic fields. This yields key structured outputs, including <strong>high-density semantic point clouds</strong> and <strong>semantically-enriched Bird's-Eye View (BEV) maps</strong> that delineate the architectural layout.</p> <p><strong>Key contributions include:</strong> (1) A unified framework that fuses LDS SLAM 2D pose priors with visual reconstruction, solving scale ambiguity and improving geometric accuracy through spatial constraints from robot navigation data. (2) An intelligent image pair selection strategy guided by 2D occupancy maps, which significantly reduces computational overhead while maintaining reconstruction quality. (3) A pipeline from raw sensor data to semantic 3D understanding, incorporating MASt3R for feature matching, GLOMAP for global optimization, and FC-CLIP for semantic segmentation.</p> <p>The framework successfully generates semantically-labeled 3D point clouds, 2D bird's-eye-view maps, and 3D bounding boxes, providing comprehensive spatial understanding for mobile robot navigation and scene analysis. This work represents a significant advancement in bridging the gap between robotic perception and 3D scene understanding, with immediate applications in autonomous cleaning robots, indoor navigation, and spatial AI systems.</p> </div> </div> </div> </div> </section> <section class="section"> <div class="container is-max-desktop"> </div> </section> <footer class="footer"> <div class="container"> <div class="content has-text-centered"> </div> <div class="columns is-centered"> <div class="column is-8"> <div class="content"> <p> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>. This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io" rel="external nofollow noopener" target="_blank">Nerfies</a>. We sincerely thank <a href="https://keunhong.com/" rel="external nofollow noopener" target="_blank">Keunhong Park</a> for developing and open-sourcing this template. </p> </div> </div> </div> </div> </footer> </div></section> </body> </html>