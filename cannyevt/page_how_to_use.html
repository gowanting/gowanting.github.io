<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Canny-EVT: How to use</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Canny-EVT
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="index.html">The OpenGV library</a></li>  </ul>
</div>
</div><!-- top -->
<div><div class="header">
  <div class="headertitle"><div class="title">How to use</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This page gives an introduction to the usage of OpenGV including a description of the interface and explicit examples. More information can be found in [17]. However, in order to have a smooth communication and full understanding of the functionality and documentation of the library, we first need to clearly define the meaning of a couple of words in the present context.</p>
<h1><a class="anchor" id="sec_vocabulary"></a>
Vocabulary</h1>
<ul>
<li>
<b>Bearing vector</b>: A bearing vector is defined to be a 3-vector with unit norm bearing at a spatial 3D point from a camera reference frame. It has 2 degrees of freedom, which are the azimuth and elevation in the camera reference frame. Because it has only two degrees of freedom, we frequently refer to it as a 2D information. It is normally expressed in a camera reference frame. </li>
<li>
<b>Landmark</b>: A landmark describes a 3D spatial point (usually expressed in a fixed frame called world reference frame). </li>
<li>
<p class="startli"><b>Camera</b>: OpenGV assumes to be in the calibrated case, and landmark measurements are always given in form of bearing vectors in a camera frame. A camera therefore denotes a camera reference frame with a set of bearing vectors, all pointing from the origin to landmarks. The following figure shows a camera c with bearing vectors (in red). The bearing vectors are all lying on the unit-sphere centered around the camera.</p>
<div class="image">
<img src="central.png" alt=""/>
</div>
 <p class="endli"></p>
</li>
<li>
<p class="startli"><b>Viewpoint</b>: You will notice that the documentation of the code very frequently talks about viewpoints instead of cameras. One of the advantages of OpenGV is that it can transparently handle both the central and the non-central case. The viewpoint is a generalization of a camera, and can contain an arbitrary number of cameras each one having it's own landmark-measurements (e.g. bearing vectors). A practical example of a viewpoint would be the set of images and related measurements captured by a fully-calibrated, rigid multi-camera rig with synchronized cameras, which therefore still represents a single (multi)-snapshot (i.e. viewpoint). Each camera has its own transformation to the viewpoint frame. In the central case the viewpoint simply contains a single camera with an identity transformation. The most general case-the generalized camera-can also be described by the viewpoint. Each bearing vector would then have it's own camera and related transformation. A generalized camera would hence be represented by an exhaustive multi-camera system. The following image shows a viewpoint vp (in blue) with three cameras c, c', and c'', each one containing its own bearing vector measurements.</p>
<div class="image">
<img src="noncentral.png" alt=""/>
</div>
 <p class="endli"></p>
</li>
<li>
<b>Pose</b>: By a pose, we understand here the position and orientation of a viewpoint, either with respect to a fixed spatial reference frame called the "world" reference frame, or with respect to another viewpoint. </li>
<li>
<b>Absolute Pose</b>: By absolute pose, we understand the pose of a viewpoint in the world reference frame. </li>
<li>
<b>Relative Pose</b>: By relative pose, we understand the pose of a viewpoint with respect to another viewpoint. </li>
<li>
<b>Correspondence</b>: By a correspondence, we understand a pair of bearing-vectors pointing at the same landmark from different viewpoints (2D-2D correspondence), a bearing vector and a world-point it is pointing at (2D-3D correspondence), or a pair of expressions of the same landmark in different frames (3D-3D correspondence). </li>
</ul>
<h1><a class="anchor" id="sec_organization"></a>
Organization of the library</h1>
<p>The library-structure is best analyzed at the hand of the namespace or directory hierarchy (as a matter of fact, there is no difference between the two): </p><ul>
<li>
opengv: contains generic things such as the types used throughout the library. </li>
<li>
opengv/math: contains a bunch of math functions that are used in different algorithms, mainly for root-finding and rotation-related stuff. </li>
<li>
opengv/absolute_pose: contains the absolute-pose methods. methods.hpp is the main-header that contains the method-declarations. You can also find a bunch of adapters here for interfacing with the algorithms (explained in the next section). The sub-folder modules contains declarations of internal methods. </li>
<li>
opengv/relative_pose: contains the relative-pose methods. methods.hpp is the main-header that contains the method-declarations. You can also find a bunch of adapters here for interfacing with the algorithms (explained in the next section). The sub-folder modules contains declarations of internal methods. </li>
<li>
opengv/point_cloud: contains the point-cloud alignment methods. Again, methods.hpp contains the declarations, and the folder contains adapters for interfacing (explained below). </li>
<li>
opengv/triangulation: contains the triangulation methods. </li>
<li>
opengv/sac: contains base-classes for sample-consensus methods and problems. So far, only the Ransac algorithm is implemented. </li>
<li>
opengv/sac_problems: contains sample-consensus problems derived from the base-class. Implements sample-consensus problems for point-cloud alignment and central as well as non-central absolute and relative-pose estimation. </li>
</ul>
<h1><a class="anchor" id="sec_interface"></a>
Interface</h1>
<p>You will quickly notice that all methods in OpenGV use a variable called adapter as a function-call parameter. OpenGV is designed based on the adapter pattern. "Adapters" in OpenGV are used as "visitors" to all geometric vision methods. They contain the landmarks, bearing-vectors, poses, correspondences etc. used as an input to the different methods (or references to the alike), and allow to access those elements with a unified interface. Adapters are derived from a base-class that defines the unified interface and they have to implement the related functions for accessing bearing-vectors, world-points, camera-transformations, viewpoint-poses, etc. There are three adapter-base-classes:</p>
<ul>
<li>
<b>AbsoluteAdapterBase</b>: Base-class for adapters holding 2D-3D correspondences for absolute-pose methods. </li>
<li>
<b>RelativeAdapterBase</b>: Base-class for adapters holding 2D-2D correspondences for relative-pose methods. </li>
<li>
<b>PointCloudAdapterBase</b>: Base-class for adapters holding 3D-3D correspondences for point-cloud alignment methods. </li>
</ul>
<p>The derived adapters have the task of transforming the data from the user-format to OpenGV types. This gives the library great flexibility. Users have to implement only a couple of adapters for the specific data-format they are using, and can then access the full functionality of the library. OpenGV currently contains adapters that simply hold references to OpenGV types (no transformation needed) plus adapters for mexArrays used within the Matlab-interface. Further adapters are planned, such as for instance an adapter for OpenCV keypoint and match-types including a camera model. The user would then be able to chose whether normalization of keypoints is done "on-demand" or "once for all" at the beginning, which is more efficient in sample-consensus problems.</p>
<p>Note that adapters containing the tag "Central" in their name are adapters for a single camera (i.e. view-points with only one camera having identity transformation). Adapters having the tag "Noncentral" in their name are meant for view-points with multiple cameras (e.g., multi-camera systems, generalized cameras).</p>
<p>Please check out the doxygen documentation on the above base-classes, they contain important documentation on the functions that need to be overloaded for a proper implementation of an adapter.</p>
<h1><a class="anchor" id="sec_conventions"></a>
Conventions, problem types, and examples</h1>
<p>As already mentioned, the entire library is assuming calibrated cameras/viewpoints, and it operates with 3D unit bearing vectors expressed in the camera frame. Calibrated means that the configuration of the multi-camera system (i.e. the inter-camera transformations) is known. The following introduces the different problems that can be solved with the library, and outlines the conventions for transformations (translations and rotations) in their context. <b>Note that all problems have solutions for both the minimal and non-minimal cases, and may also be solved as sample-consensus or non-linear optimization problems</b>. The code samples are mostly taken from the test files, which you can compile along with the library by setting BUILD_TESTS in CMakeLists.txt to ON.</p>
<ul>
<li>
<p class="startli"><b>Central absolute pose:</b> The central absolute pose problem consists of finding the pose of a camera (e.g. viewpoint with a single camera) given a number of 2D-3D correspondences between bearing vectors in the camera frame and points in the world frame. The seeked transformation is given by the position <picture><source srcset="form_0_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ \mathbf{t}_{c} $" src="form_0.png"/></picture> of the camera seen from the world frame and the rotation <picture><source srcset="form_1_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ \mathbf{R}_{c} $" src="form_1.png"/></picture> from the camera to the world frame. This is what the algorithms return (or part of it), and what the adapters can hold as known or prior information.</p>
<div class="image">
<img src="absolute_central.png" alt=""/>
</div>
 <p class="interli">The minimal variants are p2p (a solution for position if rotation is known), p3p_kneip [1], p3p_gao [2], and UPnP [19]. The non-minimal variants are epnp [4], and UPnP [19]. The non-linear optimization variant is called optimize_nonlinear. Here's how to use them:</p>
<div class="fragment"><div class="line"><span class="comment">// create the central adapter</span></div>
<div class="line">absolute_pose::CentralAbsoluteAdapter adapter(</div>
<div class="line">    bearingVectors, points );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Kneip&#39;s P2P (uses rotation from adapter)</span></div>
<div class="line">adapter.setR( knownRotation );</div>
<div class="line">translation_t p2p_translation =</div>
<div class="line">    absolute_pose::p2p( adapter, indices1 );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Kneip&#39;s P3P</span></div>
<div class="line">transformations_t p3p_kneip_transformations =</div>
<div class="line">    absolute_pose::p3p_kneip( adapter, indices2 );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Gao&#39;s P3P</span></div>
<div class="line">transformations_t p3p_gao_transformations =</div>
<div class="line">    absolute_pose::p3p_gao( adapter, indices2 );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Lepetit&#39;s Epnp (using all correspondences)</span></div>
<div class="line">transformation_t epnp_transformation =</div>
<div class="line">    absolute_pose::epnp(adapter);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// UPnP (using all correspondences)</span></div>
<div class="line">transformations_t upnp_transformations =</div>
<div class="line">    absolute_pose::upnp(adapter);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// UPnP (using three correspondences)</span></div>
<div class="line">transformations_t upnp_transformations =</div>
<div class="line">    absolute_pose::upnp( adapter, indices2 );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// non-linear optimization (using all correspondences)</span></div>
<div class="line">adapter.sett(initial_translation);</div>
<div class="line">adapter.setR(initial_rotation);</div>
<div class="line">transformation_t nonlinear_transformation =</div>
<div class="line">    absolute_pose::optimize_nonlinear(adapter);</div>
</div><!-- fragment --><p class="interli">p3p_kneip, p3p_gao, and epnp can also be used within a sample consensus context. The following shows how to do it:</p>
<div class="fragment"><div class="line"><span class="comment">// create the central adapter</span></div>
<div class="line">absolute_pose::CentralAbsoluteAdapter adapter(</div>
<div class="line">    bearingVectors, points );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// create a Ransac object</span></div>
<div class="line">sac::Ransac&lt;sac_problems::absolute_pose::AbsolutePoseSacProblem&gt; ransac;</div>
<div class="line"> </div>
<div class="line"><span class="comment">// create an AbsolutePoseSacProblem</span></div>
<div class="line"><span class="comment">// (algorithm is selectable: KNEIP, GAO, or EPNP)</span></div>
<div class="line">std::shared_ptr&lt;sac_problems::absolute_pose::AbsolutePoseSacProblem&gt;</div>
<div class="line">    absposeproblem_ptr(</div>
<div class="line">    <span class="keyword">new</span> sac_problems::absolute_pose::AbsolutePoseSacProblem(</div>
<div class="line">    adapter, sac_problems::absolute_pose::AbsolutePoseSacProblem::KNEIP ) );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// run ransac</span></div>
<div class="line">ransac.sac_model_ = absposeproblem_ptr;</div>
<div class="line">ransac.threshold_ = threshold;</div>
<div class="line">ransac.max_iterations_ = maxIterations;</div>
<div class="line">ransac.computeModel();</div>
<div class="line"> </div>
<div class="line"><span class="comment">// get the result</span></div>
<div class="line">transformation_t best_transformation =</div>
<div class="line">    ransac.model_coefficients_;</div>
</div><!-- fragment --><p class="interli">These examples are taken from test_absolute_pose.cpp and test_absolute_pose_sac.cpp.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b>Non-central absolute pose:</b> The non-central absolute pose problem consists of finding the pose of a viewpoint given a number of 2D-3D correspondences between bearing vectors in multiple camera frames and points in the world frame. The seeked transformation is given by the position <picture><source srcset="form_2_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ \mathbf{t}_{vp} $" src="form_2.png"/></picture> of the viewpoint seen from the world frame and the rotation <picture><source srcset="form_3_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ \mathbf{R}_{vp} $" src="form_3.png"/></picture> from the viewpoint to the world frame. This is what the algorithms return, and what the adapters can hold as known or prior information.</p>
<div class="image">
<img src="absolute_noncentral.png" alt=""/>
</div>
 <p class="interli">The minimal variant is gp3p, and the non-minimal variant is gpnp [3]. UPnP can be used for both the minimal and the non-minimal case [19]. The non-linear optimization variant is still optimize_nonlinear (it handles both cases). Here's how to use them:</p>
<div class="fragment"><div class="line"><span class="comment">// create the non-central adapter</span></div>
<div class="line">absolute_pose::NoncentralAbsoluteAdapter adapter(</div>
<div class="line">    bearingVectors,</div>
<div class="line">    camCorrespondences,</div>
<div class="line">    points,</div>
<div class="line">    camOffsets,</div>
<div class="line">    camRotations );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Kneip&#39;s GP3P</span></div>
<div class="line">transformations_t gp3p_transformations =</div>
<div class="line">    absolute_pose::gp3p( adapter, indices1 );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Kneip&#39;s GPNP (using all correspondences)</span></div>
<div class="line">transformation_t gpnp_transformation =</div>
<div class="line">    absolute_pose::gpnp(adapter);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// UPnP (using all correspondences)</span></div>
<div class="line">transformations_t upnp_transformations =</div>
<div class="line">    absolute_pose::upnp( adapter );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// UPnP (using only 3 correspondences)</span></div>
<div class="line">transformations_t upnp_transformations_3 =</div>
<div class="line">    absolute_pose::upnp( adapter, indices1 );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// non-linear optimization</span></div>
<div class="line">adapter.sett(initial_translation);</div>
<div class="line">adapter.setR(initial_rotation);</div>
<div class="line">transformation_t nonlinear_transformation =</div>
<div class="line">    absolute_pose::optimize_nonlinear(adapter);</div>
</div><!-- fragment --><p class="interli">gp3p can also be used within a sample-consensus context. It remains an AbsolutePoseSacProblem, this one is usable for both the central and the non-central case. We simply have to set algorithm to GP3P:</p>
<div class="fragment"><div class="line"><span class="comment">// create the non-central adapter</span></div>
<div class="line">absolute_pose::NoncentralAbsoluteAdapter adapter(</div>
<div class="line">    bearingVectors,</div>
<div class="line">    camCorrespondences,</div>
<div class="line">    points,</div>
<div class="line">    camOffsets,</div>
<div class="line">    camRotations );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// create a RANSAC object</span></div>
<div class="line">sac::Ransac&lt;sac_problems::absolute_pose::AbsolutePoseSacProblem&gt; ransac;</div>
<div class="line"> </div>
<div class="line"><span class="comment">// create a absolute-pose sample consensus problem (using GP3P as an algorithm)</span></div>
<div class="line">std::shared_ptr&lt;sac_problems::absolute_pose::AbsolutePoseSacProblem&gt;</div>
<div class="line">    absposeproblem_ptr(</div>
<div class="line">    <span class="keyword">new</span> sac_problems::absolute_pose::AbsolutePoseSacProblem(</div>
<div class="line">    adapter, sac_problems::absolute_pose::AbsolutePoseSacProblem::GP3P ) );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// run ransac</span></div>
<div class="line">ransac.sac_model_ = absposeproblem_ptr;</div>
<div class="line">ransac.threshold_ = threshold;</div>
<div class="line">ransac.max_iterations_ = maxIterations;</div>
<div class="line">ransac.computeModel();</div>
<div class="line"> </div>
<div class="line"><span class="comment">// get the result</span></div>
<div class="line">transformation_t best_transformation =</div>
<div class="line">    ransac.model_coefficients_;</div>
</div><!-- fragment --><p class="interli">These examples are taken from test_noncentral_absolute_pose.cpp and test_noncentral_absolute_pose_sac.cpp.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b>Central relative pose:</b> The central relative pose problem consists of finding the pose of a camera (e.g. viewpoint with a single camera) with respect to a different camera given a number of 2D-2D correspondences between bearing vectors in the camera frames. The seeked transformation is given by the position <picture><source srcset="form_4_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ \mathbf{t}_{c'}^{c} $" src="form_4.png"/></picture> of the second camera seen from the first one and the rotation <picture><source srcset="form_5_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ \mathbf{R}_{c'}^{c} $" src="form_5.png"/></picture> from the second camera back to the first camera frame. This is what the algorithms return (or part of it), and what the adapters can hold as known or prior information.</p>
<div class="image">
<img src="relative_central.png" alt=""/>
</div>
 <p class="interli">There are many central relative-pose algorithms in the library. The minimal variants are twopt (in case the rotation is known), twopt_rotationOnly (in case there is only rotational change, and using only two points), fivept_stewenius [5], fivept_nister [6], and fivept_kneip [7]. The libary also contains non-minimal variants, namely rotationOnly (in case of pure-rotation change), sevenpt [8], eightpt [9,10] and the new eigensolver [11] methods. All of them except twopt, twopt_rotationOnly, and fivept_kneip can be used for an arbitrary number of correspondences (of course at least the minimal number). The non-linear optimization variant is again called optimize_nonlinear. Here's how to use most of them (we assume a regular situation here, and thus omit the rotationOnly algorithms):</p>
<div class="fragment"><div class="line"><span class="comment">// create the central relative adapter</span></div>
<div class="line">relative_pose::CentralRelativeAdapter adapter(</div>
<div class="line">    bearingVectors1, bearingVectors2 );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Relative translation with only two point-correspondences</span></div>
<div class="line"><span class="comment">// (no or known rotation)</span></div>
<div class="line">adapter.setR(knownRotation);</div>
<div class="line">translation_t twopt_translation =</div>
<div class="line">    relative_pose::twopt( adapter, <span class="keyword">true</span>, indices1 );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Stewenius&#39; 5-point algorithm</span></div>
<div class="line">complexEssentials_t fivept_stewenius_essentials =</div>
<div class="line">    relative_pose::fivept_stewenius( adapter, indices2 );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Nister&#39;s 5-point algorithm</span></div>
<div class="line">essentials_t fivept_nister_essentials =</div>
<div class="line">    relative_pose::fivept_nister( adapter, indices2 );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Kneip&#39;s 5-point algorithm</span></div>
<div class="line">rotations_t fivept_kneip_rotations =</div>
<div class="line">    relative_pose::fivept_kneip( adapter, indices2 );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// the 7-point algorithm</span></div>
<div class="line">essentials_t sevenpt_essentials =</div>
<div class="line">    relative_pose::sevenpt( adapter, indices3 );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// the 8-point algorithm</span></div>
<div class="line">essential_t eightpt_essential =</div>
<div class="line">    relative_pose::eightpt( adapter, indices4 );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Kneip&#39;s eigensolver</span></div>
<div class="line">adapter.setR(initial_rotation);</div>
<div class="line">eigensolver_rotation =</div>
<div class="line">    relative_pose::eigensolver( adapter, indices5 );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// non-linear optimization (using all available correspondences)</span></div>
<div class="line">adapter.sett(initial_translation);</div>
<div class="line">adapter.setR(initial_rotation);</div>
<div class="line">transformation_t nonlinear_transformation =</div>
<div class="line">    relative_pose::optimize_nonlinear(adapter);</div>
</div><!-- fragment --><p class="interli">fivept_nister, fivept_stewenius, sevenpt, and eigthpt can also be used within a random-sample consensus scheme. It is done as follows:</p>
<div class="fragment"><div class="line"><span class="comment">// create the central relative adapter</span></div>
<div class="line">relative_pose::CentralRelativeAdapter adapter(</div>
<div class="line">    bearingVectors1, bearingVectors2 );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// create a RANSAC object</span></div>
<div class="line">sac::Ransac&lt;sac_problems::relative_pose::CentralRelativePoseSacProblem&gt; ransac;</div>
<div class="line"> </div>
<div class="line"><span class="comment">// create a CentralRelativePoseSacProblem</span></div>
<div class="line"><span class="comment">// (set algorithm to STEWENIUS, NISTER, SEVENPT, or EIGHTPT)</span></div>
<div class="line">std::shared_ptr&lt;sac_problems::relative_pose::CentralRelativePoseSacProblem&gt;</div>
<div class="line">    relposeproblem_ptr(</div>
<div class="line">    <span class="keyword">new</span> sac_problems::relative_pose::CentralRelativePoseSacProblem(</div>
<div class="line">    adapter,</div>
<div class="line">    sac_problems::relative_pose::CentralRelativePoseSacProblem::NISTER ) );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// run ransac</span></div>
<div class="line">ransac.sac_model_ = relposeproblem_ptr;</div>
<div class="line">ransac.threshold_ = threshold;</div>
<div class="line">ransac.max_iterations_ = maxIterations;</div>
<div class="line">ransac.computeModel();</div>
<div class="line"> </div>
<div class="line"><span class="comment">// get the result</span></div>
<div class="line">transformation_t best_transformation =</div>
<div class="line">    ransac.model_coefficients_;</div>
</div><!-- fragment --><p class="interli">These examples are taken from test_relative_pose.cpp and test_relative_pose_sac.cpp. There are also sample consensus problems for the case of pure-rotation, known rotation, or the eigensolver method. Feel free to explore opengv/sac_problems/relative_pose.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b>Non-central relative pose:</b> The non-central relative pose problem consists of finding the pose of a viewpoint with respect to a different viewpoint given a number of 2D-2D correspondences between bearing vectors in multiple camera frames. The seeked transformation is given by the position <picture><source srcset="form_6_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ \mathbf{t}_{vp'}^{vp} $" src="form_6.png"/></picture> of the second viewpoint seen from the first one and the rotation <picture><source srcset="form_7_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ \mathbf{R}_{vp'}^{vp} $" src="form_7.png"/></picture> from the second viewpoint back to the first viewpoint frame. This is what the algorithms return (or part of it), and what the adapters can hold as known or prior information.</p>
<div class="image">
<img src="relative_noncentral.png" alt=""/>
</div>
 <p class="interli">There are three non-central relative pose methods in the library, the 17-point algorithm by Li [12], the 6-point method by Stewenius [16], and the new generalized eigensolver [18]. The 17-point algorithm as well as the generalized eigensolver can be used with an arbitrary number of points. The 6-point algorithm is usable with only 6-points exactly. "optimize_nonlinear" is again able to also handle the non-central case. Here's how to use these methods:</p>
<div class="fragment"><div class="line"><span class="comment">// create the non-central relative adapter</span></div>
<div class="line">relative_pose::NoncentralRelativeAdapter adapter(</div>
<div class="line">    bearingVectors1,</div>
<div class="line">    bearingVectors2,</div>
<div class="line">    camCorrespondences1,</div>
<div class="line">    camCorrespondences2,</div>
<div class="line">    camOffsets,</div>
<div class="line">    camRotations );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// 6-point algorithm</span></div>
<div class="line">rotations_t sixpt_rotations =</div>
<div class="line">    relative_pose::sixpt( adapter, indices );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// generalized eigensolver (over all points)</span></div>
<div class="line">geOutput_t output;</div>
<div class="line">relative_pose::ge(adapter,output);</div>
<div class="line">translation_t ge_translation = output.translation.block&lt;3,1&gt;(0,0);</div>
<div class="line">rotation_t ge_rotation = output.rotation;</div>
<div class="line"> </div>
<div class="line"><span class="comment">// 17-point algorithm</span></div>
<div class="line">transformation_t seventeenpt_transformation =</div>
<div class="line">    relative_pose::seventeenpt( adapter, indices );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// non-linear optimization (using all available correspondences)</span></div>
<div class="line">adapter.sett(initial_translation);</div>
<div class="line">adapter.setR(initial_rotation);</div>
<div class="line">transformation_t nonlinear_transformation =</div>
<div class="line">    relative_pose::optimize_nonlinear(adapter);</div>
</div><!-- fragment --><p class="interli">All algorithms are also available in a sample-consensus scheme:</p>
<div class="fragment"><div class="line"><span class="comment">// create the non-central relative adapter</span></div>
<div class="line">relative_pose::NoncentralRelativeAdapter adapter(</div>
<div class="line">    bearingVectors1,</div>
<div class="line">    bearingVectors2,</div>
<div class="line">    camCorrespondences1,</div>
<div class="line">    camCorrespondences2,</div>
<div class="line">    camOffsets,</div>
<div class="line">    camRotations );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// create a RANSAC object</span></div>
<div class="line">sac::Ransac&lt;sac_problems::relative_pose::NoncentralRelativePoseSacProblem&gt;</div>
<div class="line">    ransac;</div>
<div class="line"> </div>
<div class="line"><span class="comment">// create a NoncentralRelativePoseSacProblem</span></div>
<div class="line">std::shared_ptr&lt;</div>
<div class="line">    sac_problems::relative_pose::NoncentralRelativePoseSacProblem&gt;</div>
<div class="line">    relposeproblem_ptr(</div>
<div class="line">    <span class="keyword">new</span> sac_problems::relative_pose::NoncentralRelativePoseSacProblem(</div>
<div class="line">    adapter,</div>
<div class="line">    sac_problems::relative_pose::NoncentralRelativePoseSacProblem::SEVENTEENPT)</div>
<div class="line">    );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// run ransac</span></div>
<div class="line">ransac.sac_model_ = relposeproblem_ptr;</div>
<div class="line">ransac.threshold_ = threshold;</div>
<div class="line">ransac.max_iterations_ = maxIterations;</div>
<div class="line">ransac.computeModel();</div>
<div class="line"> </div>
<div class="line"><span class="comment">// get the result</span></div>
<div class="line">transformation_t best_transformation =</div>
<div class="line">    ransac.model_coefficients_;</div>
</div><!-- fragment --><p class="interli">These examples are taken from test_noncentral_relative_pose.cpp and test_noncentral_relative_pose_sac.cpp. Simply set SEVENTEENPT to GE or SIXPT in order to use the alternative algorithms.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b>Triangulation of points:</b> OpenGV contains two methods for triangulating points. They are currently only designed for the central case, and compute the position of a point expressed in the first camera given a 2D-2D correspondence between bearing vectors from two cameras. The methods reuse the relative adapter, which need to hold the transformation between the cameras given by the position <picture><source srcset="form_4_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ \mathbf{t}_{c'}^{c} $" src="form_4.png"/></picture> of the second camera seen from the first one and the rotation <picture><source srcset="form_5_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ \mathbf{R}_{c'}^{c} $" src="form_5.png"/></picture> from the second camera back to the first camera frame.</p>
<div class="image">
<img src="triangulation_central.png" alt=""/>
</div>
 <p class="interli">There are two methods, triangulate (linear) and triangulate2 (a fast non-linear approximation). They are used as follows:</p>
<div class="fragment"><div class="line"><span class="comment">// create a central relative adapter</span></div>
<div class="line"><span class="comment">// (immediately pass translation and rotation)</span></div>
<div class="line">relative_pose::CentralRelativeAdapter adapter(</div>
<div class="line">    bearingVectors1,</div>
<div class="line">    bearingVectors2,</div>
<div class="line">    translation,</div>
<div class="line">    rotation );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// run method 1</span></div>
<div class="line">point_t point =</div>
<div class="line">    triangulation::triangulate( adapter, index );</div>
<div class="line"> </div>
<div class="line"><span class="comment">//run method 2</span></div>
<div class="line">point_t point =</div>
<div class="line">    triangulation::triangulate2( adapter, index );</div>
</div><!-- fragment --><p class="interli">The example is taken from test_triangulation.cpp.</p>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b>Alignment of two point-clouds:</b> OpenGV also contains a method for aligning point-clouds. It is currently only designed for the central case, and computes the transformation between two frames given 3D-3D correspondences between points expressed in the two frames (here denoted by c and c', although it ain't necessarily need to be cameras anymore). The method returns the transformation between the frames given by the position <picture><source srcset="form_4_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ \mathbf{t}_{c'}^{c} $" src="form_4.png"/></picture> of the second frame seen from the first one and the rotation <picture><source srcset="form_5_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ \mathbf{R}_{c'}^{c} $" src="form_5.png"/></picture> from the second frame back to the first frame.</p>
<div class="image">
<img src="point_cloud.png" alt=""/>
</div>
 <p class="interli">The method is called threept_arun, and it can be used for an arbitrary number of points (minimum three). There is also a non-linear optimization method again called optimize_nonlinear. The methods are used as follows:</p>
<div class="fragment"><div class="line"><span class="comment">// create the 3D-3D adapter</span></div>
<div class="line">point_cloud::PointCloudAdapter adapter(</div>
<div class="line">    points1, points2 );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// run threept_arun</span></div>
<div class="line">transformation_t threept_transformation =</div>
<div class="line">    point_cloud::threept_arun( adapter, indices );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// run the non-linear optimization over all correspondences</span></div>
<div class="line">transformation_t nonlinear_transformation =</div>
<div class="line">    point_cloud::optimize_nonlinear(adapter);</div>
</div><!-- fragment --><p class="interli">There is also a sample-consensus problem for the point-cloud alignment. It is set up as follows:</p>
<div class="fragment"><div class="line"><span class="comment">// create a 3D-3D adapter</span></div>
<div class="line">point_cloud::PointCloudAdapter adapter(</div>
<div class="line">    points1, points2 );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// create a RANSAC object</span></div>
<div class="line">sac::Ransac&lt;sac_problems::point_cloud::PointCloudSacProblem&gt; ransac;</div>
<div class="line"> </div>
<div class="line"><span class="comment">// create the sample consensus problem</span></div>
<div class="line">std::shared_ptr&lt;sac_problems::point_cloud::PointCloudSacProblem&gt;</div>
<div class="line">    relposeproblem_ptr(</div>
<div class="line">    <span class="keyword">new</span> sac_problems::point_cloud::PointCloudSacProblem(adapter) );</div>
<div class="line"> </div>
<div class="line"><span class="comment">// run ransac</span></div>
<div class="line">ransac.sac_model_ = relposeproblem_ptr;</div>
<div class="line">ransac.threshold_ = threshold;</div>
<div class="line">ransac.max_iterations_ = maxIterations;</div>
<div class="line">ransac.computeModel(0);</div>
<div class="line"> </div>
<div class="line"><span class="comment">// return the result</span></div>
<div class="line">transformation_t best_transformation =</div>
<div class="line">    ransac.model_coefficients_;</div>
</div><!-- fragment --><p class="interli">These examples are taken from test_point_cloud.cpp and test_point_cloud_sac.cpp.</p>
<p class="endli"></p>
</li>
</ul>
<p>Note that there are more unit-tests in the test-directory. It shows the usage of all the methods contained in the library.</p>
<h1><a class="anchor" id="sec_threshold"></a>
Some words about the sample-consensus-classes</h1>
<p>All the above mentioned Ransac-methods make use of a number of super-classes such that only the basic functions need to be implemented in the derived SacProblem (SampleConsensusProblem). The basic functions are responsible for getting valid samples for model instantiation, model instantiation itself, as well as model verification. <b>SamplesConsensusProblem</b> is the base-class for any problem we want to solve, and contains a virtual interface for the basic methods that need to be implemented. The base-class <b>SampleConsensus</b> is then for the sample-consensus method itself, calling the basic functions. So far only the <b>Ransac</b> is implemented [15].</p>
<h2><a class="anchor" id="sec_ransac"></a>
Ransac threshold</h2>
<p>Since the entire library is operating in 3D, we also need a way to compute and threshold reprojection errors in 3D. What we are looking at is the angle <picture><source srcset="form_8_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ q $" src="form_8.png"/></picture> between the original bearing-vector <picture><source srcset="form_9_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ \mathbf{f}_{meas} $" src="form_9.png"/></picture> and the reprojected one <picture><source srcset="form_10_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ \mathbf{f}_{repr} $" src="form_10.png"/></picture>. By adopting a certain threshold angle <picture><source srcset="form_11_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ q_{threshold} $" src="form_11.png"/></picture>, we hence constrain the <picture><source srcset="form_10_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ \mathbf{f}_{repr} $" src="form_10.png"/></picture> to lie within a cone of axis <picture><source srcset="form_9_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ \mathbf{f}_{meas} $" src="form_9.png"/></picture> and of opening angle <picture><source srcset="form_11_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ q_{threshold} $" src="form_11.png"/></picture>.</p>
<div class="image">
<img src="reprojectionError.png" alt=""/>
</div>
 <p>The threshold-angle <picture><source srcset="form_11_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ q_{threshold} $" src="form_11.png"/></picture> can be easily obtained from classical reprojection error-thresholds expressed in pixels <picture><source srcset="form_12_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ \psi $" src="form_12.png"/></picture> by assuming a certain focal length <picture><source srcset="form_13_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ l $" src="form_13.png"/></picture>. We then have <picture><source srcset="form_14_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ q_{threshold} = \arctan{\frac{\psi}{l}} $" src="form_14.png"/></picture>.</p>
<p>The threshold we are using in the end is still not quite this one, but a value derived from it in analogy with the computation of reprojection errors. The most efficient way to compute a "reprojection error" is given by taking the scalar product of <picture><source srcset="form_9_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ \mathbf{f}_{meas} $" src="form_9.png"/></picture> and <picture><source srcset="form_10_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ \mathbf{f}_{repr} $" src="form_10.png"/></picture>, which equals to <picture><source srcset="form_15_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ \cos q $" src="form_15.png"/></picture>. Since this value is between -1 and 1, and we actually want an error that minimizes to 0, we take <picture><source srcset="form_16_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ \epsilon = 1 - \mathbf{f}_{meas}^{T}\mathbf{f}_{repr} = 1 - \cos q $" src="form_16.png"/></picture>. The threshold error is therefore given by</p>
<p><picture><source srcset="form_17_dark.png" media="(prefers-color-scheme: dark)"/><img class="formulaInl" alt="$ \epsilon_{threshold} = 1 - \cos{q_{threshold}} = 1 - \cos({\arctan{\frac{\psi}{l}}}) $" src="form_17.png"/></picture></p>
<p>In the ransac-examples in the test-folder, you will often see something like this.</p>
<div class="fragment"><div class="line">ransac.threshold_ = 1.0 - cos(atan(sqrt(2.0)*0.5/800.0));</div>
</div><!-- fragment --><p>This notably corresponds to the above computation of our "reprojection-error"-threshold, with a focal length of 800.0 and a reprojection error in pixels of 0.5*sqrt(2.0).</p>
<h1><a class="anchor" id="sec_multi"></a>
The "Multi"-stuff</h1>
<p>As you go deeper into the code you might notice that there are a number of elements (mostly in the relative-pose context) that contain the tag "multi" in their name. The adapter base-class used here is called <b>RelativeMultiAdapterBase</b>. The idea of this adapter is to hold multiple sets of bearing-vector correspondences originating from pairs of cameras. A pair of cameras is, as the name says, a set of two cameras in different viewpoints. The correspondences are accessed via a multi-index (a pair-index referring to a specific pair of cameras, and a correspondence-index refering to the correspondence within the camera-pair).</p>
<p>Subsets of camera-pairs can be identified in a number of problems, such as </p><ul>
<li>
<p class="startli">Non-central relative pose (2 viewpoints): Non-central relative pose problems involving two viewpoints typically originate from motion-estimation with multi-camera rigs. In the special situation where the cameras are pointing in different directions, and where the motion between the viewpoints is not too big (a practically very relevant case), the correspondences are typically originating from the same camera in both viewpoints. We therefore can do a camera-wise grouping of the correspondences in the multi-camera system. The following situation contains four pairs given by the black, green, blue, and orange camera in both viewpoints:</p>
<div class="image">
<img src="nonoverlapping.png" alt=""/>
</div>
 <p class="endli"></p>
</li>
<li>
<p class="startli">Central multi-viewpoint problems: By multi-viewpoint we understand here problems that involve more than two viewpoints. As indicated below, a problem of three central viewpoints for instance allows to identify three camera-pairs as well. The number of camera-pairs in an n-view problem amounts to the combination of 2 out of n, meaning n*(n-1)/2. For the below example, we could have tha camera pairs (c,c'), (c',c''), and (c'',c). The first pair would have a set of correspondences originating from points p1 and p4, the second one from p2 and p4, and the third one from p3 and p4.</p>
<div class="image">
<img src="multi_viewpoint.png" alt=""/>
</div>
 <p class="endli"></p>
</li>
</ul>
<p>The multi-adapters keep track of these camera-pair-wise correspondence groups. The benefit of it appears when moving towards random sample-consensus schemes. Have a look at the "opengv/sac/"-folder, it contains the <b>MultiSampleConsensus</b>, <b>MultiRansac</b>, and <b>MultiSampleConsensusProblem</b> classes. They employ the multi-indices, and the derived MultiSampleConsensusProblems exploit the fact that the correspondences are grouped:</p>
<ul>
<li>
The <b>MultiNoncentralRelativePoseSacProblem</b> is for non-central, non-overlapping viewpoints with little change, and exploits the grouping in order to do homogeneous sampling of correspondences over the cameras. As an example, imagine we are computing the relative pose of a non-overlapping multi-camera rig with two cameras facing opposite directions. In terms of accuracy, it doesn't make sense to sample 16-points in one camera and one point in the other. We preferrably would like to sample 8 points in one camera, and 9 in the other. This is exactly what MultiNoncentralRelativePoseSacProblem is able to do. It uses the derived adapter <b>NoncentralRelativeMultiAdapter</b>. </li>
<li>
In the multi viewpoint case, one could of course solve a central relative pose problem for each camera-pair individually. The idea of <b>MultiCentralRelativePoseSacProblem</b> is to benefit from a joint solution of multiple relative-pose problems. In the above three-view problem for instance, we can exploit additional constraints around the individual transformations such as cycles of rotations returning identity, and cycles of translations returning zero. The corresponding adapter is called <b>CentralRelativeMultiAdapter</b>. </li>
</ul>
<p>All this stuff is highly experimental, so you probably shouldn't pay too much attention to it for the moment ;) </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
